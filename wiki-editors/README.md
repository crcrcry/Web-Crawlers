# Wiki-Spider
- 维基百科 Wikipedia Editors 数据爬虫
- wiki 包含 3000 万注册用户、4000 万文章

## 原理
- 维基百科 Special pages 中包含维基的数据统计，包括 All pages 和 All users（分红名与非红名）。
- 用户和文章都可获取所有用户信息。但文章数量多三分之一，请求次数也多，而且无法根据时间筛选，所以选择根据用户爬取。
- 通过 All users 获取每位 user 的某时间的贡献信息。
- 所有用户和所有贡献的数据信息，一边算一边存储于文件 or 数据库，存储于变量中是不可能的，数据太多会耗尽内存。

## wikidata
- 维基百科的请求速度太慢
- wikidata 2018-1 有 28 G 数据，且只包含 wiki 中的条目和属性，不包含用户数据，无法使用

## improve
- 先做第一步好了
- 第二步，找个框架研究下，来实现并行